{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_det_curve(target_scores, nontarget_scores):\n",
    "    n_scores = target_scores.size + nontarget_scores.size\n",
    "    all_scores = np.concatenate((target_scores, nontarget_scores))\n",
    "    labels = np.concatenate((np.ones(target_scores.size), np.zeros(nontarget_scores.size)))\n",
    "\n",
    "    # Sort labels based on scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = nontarget_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / target_scores.size))  # false rejection rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / nontarget_scores.size))  # false acceptance rates\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))  # Thresholds are the sorted scores\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "def compute_eer(target_scores, nontarget_scores):\n",
    "    target_scores = np.array(target_scores).flatten()\n",
    "    nontarget_scores = np.array(nontarget_scores).flatten()\n",
    "    frr, far, thresholds = compute_det_curve(target_scores, nontarget_scores)\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "def eer_wrapper(score, bonafide_or_deepfake):\n",
    "    pos_samples = score[bonafide_or_deepfake == 1]\n",
    "    neg_samples = score[bonafide_or_deepfake == 0]\n",
    "    return compute_eer(pos_samples, neg_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Get all csvs\n",
    "csvs = [f for f in os.listdir(\"./baselines_csv\") if f.endswith(\".csv\")]\n",
    "print(len(csvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_datasets, singer_ids, attack_ids, bonafide_or_deepfake = [], [], [], []\n",
    "with open(\"test_groundtruth.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines.sort(key=lambda x: int(x.split()[2].split(\"_\")[-1]))\n",
    "    source_datasets, singer_ids, filename, _, attack_ids, bonafide_or_deepfakes = zip(*[line.strip().split() for line in lines])\n",
    "    bonafide_or_deepfakes = [1 if x == \"bonafide\" else 0 for x in bonafide_or_deepfakes]\n",
    "    \n",
    "    # create ground_truth reference dataframe\n",
    "    ground_truth = pd.DataFrame({\"source_dataset\": source_datasets, \"singer_id\": singer_ids, \"filename\": filename, \"attack_id\": attack_ids, \"bonafide_or_deepfake\": bonafide_or_deepfakes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis (all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.28it/s]\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for csv in tqdm(csvs):\n",
    "    data = pd.read_csv(f\"./baselines_csv/{csv}\")\n",
    "    df = data.merge(ground_truth, on=\"filename\", how=\"left\")\n",
    "\n",
    "    # compute EER for each attack\n",
    "    results = {\n",
    "        \"per_attack\": {},\n",
    "        \"per_dataset\": {},\n",
    "    }\n",
    "\n",
    "    bonafide_df = df[df[\"bonafide_or_deepfake\"] == 1]\n",
    "    # remove all ACESinger from bonafide_df\n",
    "    bonafide_df = bonafide_df[bonafide_df[\"source_dataset\"] != \"acesinger\"]\n",
    "    acesinger_bonafide_df = df[(df[\"source_dataset\"] == \"acesinger\") & (df[\"bonafide_or_deepfake\"] == 1)]\n",
    "\n",
    "    for attack_id in df[\"attack_id\"].unique():\n",
    "        if attack_id == \"-\":\n",
    "            continue\n",
    "        attack_df = df[df[\"attack_id\"] == attack_id]\n",
    "        attack_df = pd.concat([attack_df, bonafide_df])\n",
    "        if attack_id == \"A14\":\n",
    "            attack_df = pd.concat([attack_df, acesinger_bonafide_df]) # add ACESinger bonafide samples if A14 attck; this should make A14 yield the same result as the previous official evaluation script\n",
    "        eer, threshold = eer_wrapper(attack_df[\"score\"].values, attack_df[\"bonafide_or_deepfake\"].values)\n",
    "        results[\"per_attack\"][attack_id] = eer\n",
    "\n",
    "    for source_dataset in df[\"source_dataset\"].unique():\n",
    "        dataset_df = df[df[\"source_dataset\"] == source_dataset]\n",
    "        eer, threshold = eer_wrapper(dataset_df[\"score\"].values, dataset_df[\"bonafide_or_deepfake\"].values)\n",
    "        results[\"per_dataset\"][source_dataset] = eer\n",
    "\n",
    "\n",
    "    pooled_attack_eer = eer_wrapper(df[\"score\"].values, df[\"bonafide_or_deepfake\"].values)[0]\n",
    "    no_A14_df = df[df[\"attack_id\"] != \"A14\"]\n",
    "    # remove all ACESinger from no_A14_df\n",
    "    no_A14_df = no_A14_df[no_A14_df[\"source_dataset\"] != \"acesinger\"]\n",
    "    pooled_attack_discard_A14_eer = eer_wrapper(no_A14_df[\"score\"].values, no_A14_df[\"bonafide_or_deepfake\"].values)[0]\n",
    "\n",
    "    results[\"per_attack\"] = dict(sorted(results[\"per_attack\"].items(), key=lambda x: int(x[0].split(\"A\")[-1])))\n",
    "    \n",
    "    all_results.append({\n",
    "        \"csv\": csv,\n",
    "        \"EER (w/o A14)\": pooled_attack_discard_A14_eer,\n",
    "        \"EER (overall)\": pooled_attack_eer,\n",
    "        \"A09\": results[\"per_attack\"][\"A09\"],\n",
    "        \"A10\": results[\"per_attack\"][\"A10\"],\n",
    "        \"A11\": results[\"per_attack\"][\"A11\"],\n",
    "        \"A12\": results[\"per_attack\"][\"A12\"],\n",
    "        \"A13\": results[\"per_attack\"][\"A13\"],\n",
    "        \"KiSing\": results[\"per_dataset\"][\"kising\"],\n",
    "        \"M4Singer\": results[\"per_dataset\"][\"m4singer\"],\n",
    "        \"ACESinger (A14)\": results[\"per_dataset\"][\"acesinger\"],\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
